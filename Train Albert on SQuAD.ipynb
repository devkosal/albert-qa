{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using ALBERT for Question Answering - SQuAD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Updates:\n",
    "\n",
    "Things implemented:\n",
    "- Data Loading\n",
    "- Model\n",
    "- Learner\n",
    "- Prediction on new data\n",
    "- Optimizer selection\n",
    "- Segment IDs\n",
    "- F1 Score\n",
    "- Checkpoints\n",
    "- Use a more powerful machine to train\n",
    "- Gradient Accumulation\n",
    "\n",
    "TODO:\n",
    "- Consider using the sliding window approach for long sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T15:40:45.248169Z",
     "start_time": "2020-02-02T15:40:44.308349Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from src import *\n",
    "from transformers import AutoTokenizer, AlbertForQuestionAnswering\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import pickle\n",
    "import re\n",
    "import collections\n",
    "from tqdm import tqdm, trange\n",
    "from datetime import datetime\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T15:40:45.252231Z",
     "start_time": "2020-02-02T15:40:45.249605Z"
    }
   },
   "outputs": [],
   "source": [
    "logging.basicConfig()\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T15:40:45.268012Z",
     "start_time": "2020-02-02T15:40:45.254213Z"
    }
   },
   "outputs": [],
   "source": [
    "config = Config(\n",
    "    data_path = Path(\"../data/SQuAD/1.1\"), # replace with the directory containing the parsed csv files\n",
    "    output_dir = Path(\"./checkpoints\"), # for storing model weights between epochs\n",
    "    task = \"SQuAD\",\n",
    "    squad_version=\"1.1\",\n",
    "    testing=False,\n",
    "    seed = 2020,\n",
    "    model = 'albert-base-v2',\n",
    "    max_lr=5e-5,\n",
    "    optimizer=\"lamb\", # choose between 'adam' or 'lamb'\n",
    "    epochs=2,\n",
    "    use_fp16=False,\n",
    "    recreate_ds=False,\n",
    "    bs=4, \n",
    "    effective_bs=4, # set this different from bs to determine gradient accumulation steps (i.e. effective_bs/bs)\n",
    "    max_seq_len=512,\n",
    "    start_tok = \"[CLS]\",\n",
    "    end_tok = \"[SEP]\",\n",
    "    sep_tok = \"[SEP]\",\n",
    "    unk_tok_idx=1,\n",
    "    sep_idx=3,\n",
    "    pad_idx=0,\n",
    "    feat_cols = [\"question\",\"paragraph\"],\n",
    "    label_cols = \"idxs\",\n",
    "    adjustment = 2,\n",
    "    save_checkpoint = True,\n",
    "    load_checkpoint=None#\"albert-base-v2-accuracy-0.72-epoch-0-2020-01-24 18:54:15.308899\"\n",
    ")\n",
    "\n",
    "config.model_name = re.findall(r\"(.+?)-\",config.model)[0]\n",
    "\n",
    "# set optimizer\n",
    "assert config.optimizer.lower() in [\"adam\",\"lamb\"], f\"invalid optimizer in config {config.optimizer}\"\n",
    "config.opt_func = lamb_opt() if config.optimizer.lower() == \"lamb\" else adam_opt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T15:40:45.277275Z",
     "start_time": "2020-02-02T15:40:45.269744Z"
    }
   },
   "outputs": [],
   "source": [
    "# utility functions\n",
    "def remove_max_sl(df):\n",
    "    init_len = len(df)\n",
    "    df = df[df.seq_len < config.max_seq_len-2]\n",
    "    new_len = len(df)\n",
    "    print(f\"dropping {init_len - new_len} out of {init_len} questions\")\n",
    "    return df\n",
    "\n",
    "def str2tensor(s):\n",
    "    indices = re.findall(\"\\d+\",s)\n",
    "    return torch.tensor([int(indices[0]), int(indices[1])], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T15:40:46.264281Z",
     "start_time": "2020-02-02T15:40:45.278522Z"
    }
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(config.data_path/f\"train_{config.squad_version}_{config.model_name}.csv\")\n",
    "valid = pd.read_csv(config.data_path/f\"val_{config.squad_version}_{config.model_name}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T15:40:46.340991Z",
     "start_time": "2020-02-02T15:40:46.269072Z"
    }
   },
   "outputs": [],
   "source": [
    "# randomizing the order of training data\n",
    "train = train.sample(frac=1,random_state = config.seed).reset_index(drop=True)\n",
    "valid = valid.sample(frac=1, random_state = config.seed).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T15:40:46.344753Z",
     "start_time": "2020-02-02T15:40:46.342334Z"
    }
   },
   "outputs": [],
   "source": [
    "# reduce df sizes if testing\n",
    "if config.testing:\n",
    "    train = train[:1000]\n",
    "    valid = valid[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T15:40:46.380749Z",
     "start_time": "2020-02-02T15:40:46.347655Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dropping 216 out of 130319 questions\n",
      "dropping 193 out of 26232 questions\n"
     ]
    }
   ],
   "source": [
    "train, valid = remove_max_sl(train), remove_max_sl(valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T15:40:46.394908Z",
     "start_time": "2020-02-02T15:40:46.382736Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>paragraph</th>\n",
       "      <th>answer</th>\n",
       "      <th>idxs</th>\n",
       "      <th>seq_len</th>\n",
       "      <th>ans_text</th>\n",
       "      <th>is_impossible</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the mean yearly temperature in Hyderab...</td>\n",
       "      <td>Hyderabad has a tropical wet and dry climate (...</td>\n",
       "      <td>['▁26', '.', '6', '▁', '°', 'c']</td>\n",
       "      <td>[46, 52]</td>\n",
       "      <td>214</td>\n",
       "      <td>26.6 °C</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What happens after a bill is delivered to the ...</td>\n",
       "      <td>After the President signs a bill into law (or ...</td>\n",
       "      <td>['▁assigned', '▁a', '▁law', '▁number']</td>\n",
       "      <td>[62, 66]</td>\n",
       "      <td>151</td>\n",
       "      <td>assigned a law number</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What notable piece of legislation did Michael ...</td>\n",
       "      <td>In 1945, Plymouth-born Michael Foot was electe...</td>\n",
       "      <td>['▁1974', '▁health', '▁and', '▁safety', '▁at',...</td>\n",
       "      <td>[47, 54]</td>\n",
       "      <td>70</td>\n",
       "      <td>1974 Health and Safety at Work Act</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How does the Seattle ferry line compare to the...</td>\n",
       "      <td>King County Metro provides frequent stop bus s...</td>\n",
       "      <td>['▁third', '▁largest']</td>\n",
       "      <td>[146, 148]</td>\n",
       "      <td>177</td>\n",
       "      <td>third largest</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What team did Arsenal defeat at Wembley in 2015?</td>\n",
       "      <td>Arsenal reached the final of the 2007 and 2011...</td>\n",
       "      <td>['▁chelsea']</td>\n",
       "      <td>[177, 178]</td>\n",
       "      <td>198</td>\n",
       "      <td>Chelsea</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0  What is the mean yearly temperature in Hyderab...   \n",
       "1  What happens after a bill is delivered to the ...   \n",
       "2  What notable piece of legislation did Michael ...   \n",
       "3  How does the Seattle ferry line compare to the...   \n",
       "4   What team did Arsenal defeat at Wembley in 2015?   \n",
       "\n",
       "                                           paragraph  \\\n",
       "0  Hyderabad has a tropical wet and dry climate (...   \n",
       "1  After the President signs a bill into law (or ...   \n",
       "2  In 1945, Plymouth-born Michael Foot was electe...   \n",
       "3  King County Metro provides frequent stop bus s...   \n",
       "4  Arsenal reached the final of the 2007 and 2011...   \n",
       "\n",
       "                                              answer        idxs  seq_len  \\\n",
       "0                   ['▁26', '.', '6', '▁', '°', 'c']    [46, 52]      214   \n",
       "1             ['▁assigned', '▁a', '▁law', '▁number']    [62, 66]      151   \n",
       "2  ['▁1974', '▁health', '▁and', '▁safety', '▁at',...    [47, 54]       70   \n",
       "3                             ['▁third', '▁largest']  [146, 148]      177   \n",
       "4                                       ['▁chelsea']  [177, 178]      198   \n",
       "\n",
       "                             ans_text  is_impossible  \n",
       "0                             26.6 °C          False  \n",
       "1               assigned a law number          False  \n",
       "2  1974 Health and Safety at Work Act          False  \n",
       "3                       third largest          False  \n",
       "4                             Chelsea          False  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up the Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T15:40:46.411668Z",
     "start_time": "2020-02-02T15:40:46.404579Z"
    }
   },
   "outputs": [],
   "source": [
    "class TokenizerProcessor(Processor):\n",
    "    def __init__(self, tok_func, max_sl, start_tok, end_tok, pre_rules=None,post_rules=None):\n",
    "        self.tok_func,self.max_sl = tok_func,max_sl\n",
    "        self.pre_rules,self.post_rules=pre_rules,post_rules\n",
    "        self.start_tok, self.end_tok = start_tok, end_tok\n",
    "\n",
    "    def proc1(self, x): return [self.start_tok] + self.tok_func(x)[:self.max_sl-2] + [self.end_tok]\n",
    "    \n",
    "    def __call__(self, items): return tqdm([self.proc1(x) for x in items])\n",
    "\n",
    "class NumericalizeProcessor(Processor):\n",
    "    \"\"\"\n",
    "    only works with an existing vocab at the moment and min_freq is not accounted for\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab:dict, unk_tok_idx:int, min_freq=2): \n",
    "        self.vocab, self.unk_tok_idx, self.min_freq = vocab, unk_tok_idx, min_freq\n",
    "    \n",
    "    def proc1(self, x): return [self.vocab[i] if i in self.vocab else self.unk_tok_idx for i in x]\n",
    "    \n",
    "    def __call__(self, items): \n",
    "        if getattr(self, 'otoi', None) is None:\n",
    "            self.otoi = collections.defaultdict(int,{v:k for k,v in enumerate(self.vocab)})\n",
    "        return tqdm([self.proc1(x) for x in items])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T15:40:46.754326Z",
     "start_time": "2020-02-02T15:40:46.413310Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443\n",
      "DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 \"HEAD /models.huggingface.co/bert/albert-base-v2-spiece.model HTTP/1.1\" 200 0\n",
      "INFO:transformers.tokenization_utils:loading file https://s3.amazonaws.com/models.huggingface.co/bert/albert-base-v2-spiece.model from cache at /home/devkosal/.cache/torch/transformers/dd1588b85b6fdce1320e224d29ad062e97588e17326b9d05a0b29ee84b8f5f93.c81d4deb77aec08ce575b7a39a989a79dd54f321bfb82c2b54dd35f52f8182cf\n"
     ]
    }
   ],
   "source": [
    "tok = AutoTokenizer.from_pretrained(config.model)\n",
    "proc_tok = TokenizerProcessor(tok.tokenize, config.max_seq_len, config.start_tok, config.end_tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T15:40:46.788214Z",
     "start_time": "2020-02-02T15:40:46.755930Z"
    }
   },
   "outputs": [],
   "source": [
    "vocab = {tok.convert_ids_to_tokens(i):i for i in range(tok.vocab_size)}\n",
    "proc_num = NumericalizeProcessor(vocab, unk_tok_idx=config.unk_tok_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T15:40:46.803588Z",
     "start_time": "2020-02-02T15:40:46.789941Z"
    }
   },
   "outputs": [],
   "source": [
    "class QALabelProcessor(Processor):\n",
    "    def __init__(self, parse_func = noop, adjustment = 1):\n",
    "        self.parse_func = parse_func\n",
    "        self.adjustment = adjustment\n",
    "    def proc1(self, item): return self.parse_func(item) + self.adjustment\n",
    "    def __call__(self, items): return [self.proc1(item) for item in items]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T15:40:46.812723Z",
     "start_time": "2020-02-02T15:40:46.804975Z"
    }
   },
   "outputs": [],
   "source": [
    "proc_qa = QALabelProcessor(str2tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T15:40:46.821843Z",
     "start_time": "2020-02-02T15:40:46.814110Z"
    }
   },
   "outputs": [],
   "source": [
    "class TextList(ItemList):      \n",
    "    @classmethod  \n",
    "    def from_df(cls, df, feat_cols, label_col, sep_tok, test=False):\n",
    "        feat_cols = listify(feat_cols)\n",
    "        x = df[feat_cols[0]]\n",
    "        for i in range(1,len(feat_cols)):\n",
    "            x += f\" {sep_tok} \" + df[feat_cols[i]]\n",
    "        labels = cls(df[label_col]) if not test else cls([0 for _ in len(df)])\n",
    "        return cls(x,labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T15:42:37.769673Z",
     "start_time": "2020-02-02T15:40:46.823179Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:numexpr.utils:NumExpr defaulting to 8 threads.\n",
      "100%|██████████| 26039/26039 [00:00<00:00, 28386.41it/s]\n",
      "100%|██████████| 26039/26039 [00:00<00:00, 292796.62it/s]\n",
      "100%|██████████| 130103/130103 [00:04<00:00, 27882.32it/s]\n",
      "100%|██████████| 130103/130103 [00:00<00:00, 306286.94it/s]\n"
     ]
    }
   ],
   "source": [
    "if (not (config.data_path/\"squad_data_trn.pkl\").exists()) or config.recreate_ds or config.testing:\n",
    "    il_train = TextList.from_df(train,config.feat_cols,config.label_cols,config.sep_tok)\n",
    "    il_valid = TextList.from_df(valid,config.feat_cols,config.label_cols,config.sep_tok)\n",
    "\n",
    "    ll_valid = LabeledData(il_valid,il_valid.labels,proc_x = [proc_tok,proc_num], proc_y=[proc_qa])\n",
    "    ll_train = LabeledData(il_train,il_train.labels,proc_x = [proc_tok,proc_num], proc_y=[proc_qa])\n",
    "\n",
    "    # saving/loading presaved data\n",
    "    if not config.testing:\n",
    "        # save an object\n",
    "        pickle.dump(ll_train, open( config.data_path/\"squad_data_trn.pkl\", \"wb\" ) )\n",
    "        pickle.dump(ll_valid, open( config.data_path/\"squad_data_val.pkl\", \"wb\" ) )\n",
    "else:\n",
    "    # load an object\n",
    "    ll_train = pickle.load( open( config.data_path/\"squad_data_trn.pkl\", \"rb\" ) )\n",
    "    ll_valid = pickle.load( open( config.data_path/\"squad_data_val.pkl\", \"rb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T15:42:37.774355Z",
     "start_time": "2020-02-02T15:42:37.770921Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Sampler\n",
    "\n",
    "class SortSampler(Sampler):\n",
    "    def __init__(self, data_source, key): self.data_source,self.key = data_source,key\n",
    "    def __len__(self): return len(self.data_source)\n",
    "    def __iter__(self):\n",
    "        return iter(sorted(list(range(len(self.data_source))), key=self.key, reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T15:42:37.787833Z",
     "start_time": "2020-02-02T15:42:37.775937Z"
    }
   },
   "outputs": [],
   "source": [
    "class SortishSampler(Sampler):\n",
    "    def __init__(self, data_source, key, bs):\n",
    "        self.data_source,self.key,self.bs = data_source,key,bs\n",
    "\n",
    "    def __len__(self) -> int: return len(self.data_source)\n",
    "\n",
    "    def __iter__(self):\n",
    "        idxs = torch.randperm(len(self.data_source))\n",
    "        megabatches = [idxs[i:i+self.bs*50] for i in range(0, len(idxs), self.bs*50)]\n",
    "        sorted_idx = torch.cat([tensor(sorted(s, key=self.key, reverse=True)) for s in megabatches])\n",
    "        batches = [sorted_idx[i:i+self.bs] for i in range(0, len(sorted_idx), self.bs)]\n",
    "        max_idx = torch.argmax(tensor([self.key(ck[0]) for ck in batches]))  # find the chunk with the largest key,\n",
    "        batches[0],batches[max_idx] = batches[max_idx],batches[0]            # then make sure it goes first.\n",
    "        batch_idxs = torch.randperm(len(batches)-2)\n",
    "        sorted_idx = torch.cat([batches[i+1] for i in batch_idxs]) if len(batches) > 1 else LongTensor([])\n",
    "        sorted_idx = torch.cat([batches[0], sorted_idx, batches[-1]])\n",
    "        return iter(sorted_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T15:42:37.797288Z",
     "start_time": "2020-02-02T15:42:37.789189Z"
    }
   },
   "outputs": [],
   "source": [
    "def pad_collate_qa(samples, pad_idx=config.pad_idx, pad_first=False):\n",
    "    max_len = max([len(s[0]) for s in samples])\n",
    "    res = torch.zeros(len(samples), max_len).long() + pad_idx\n",
    "    for i,s in enumerate(samples):\n",
    "        if pad_first: res[i, -len(s[0]):] = torch.LongTensor(s[0])\n",
    "        else:         res[i, :len(s[0]) ] = torch.LongTensor(s[0])\n",
    "    return res, torch.cat([s[1].unsqueeze(0) for s in samples])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T15:42:37.806776Z",
     "start_time": "2020-02-02T15:42:37.798496Z"
    }
   },
   "outputs": [],
   "source": [
    "train_sampler = SortishSampler(ll_train.x, key=lambda t: len(ll_train[int(t)][0]), bs=config.bs)\n",
    "train_dl = DataLoader(ll_train, batch_size=config.bs, sampler=train_sampler, collate_fn=pad_collate_qa)\n",
    "\n",
    "valid_sampler = SortSampler(ll_valid.x, key=lambda t: len(ll_valid[int(t)][0]))\n",
    "valid_dl = DataLoader(ll_valid, batch_size=config.bs, sampler=valid_sampler, collate_fn=pad_collate_qa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up the Databunch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T15:42:37.817681Z",
     "start_time": "2020-02-02T15:42:37.808021Z"
    }
   },
   "outputs": [],
   "source": [
    "data = DataBunch(train_dl,valid_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T15:42:37.826531Z",
     "start_time": "2020-02-02T15:42:37.819005Z"
    }
   },
   "outputs": [],
   "source": [
    "config.sep_idx = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T15:42:37.835909Z",
     "start_time": "2020-02-02T15:42:37.827796Z"
    }
   },
   "outputs": [],
   "source": [
    "def set_segments(x,sep_idx=config.sep_idx):\n",
    "    res = x.new_zeros(x.size())\n",
    "    for row_idx, row in enumerate(x):\n",
    "        in_seg_1 = False\n",
    "        for val_idx,val in enumerate(row):\n",
    "            if val == sep_idx:\n",
    "                in_seg_1 = True\n",
    "            if in_seg_1: \n",
    "                res[row_idx,val_idx] = 1\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T15:42:37.846326Z",
     "start_time": "2020-02-02T15:42:37.837093Z"
    }
   },
   "outputs": [],
   "source": [
    "weights = config.output_dir/config.load_checkpoint if config.load_checkpoint else config.model\n",
    "\n",
    "class CustomAlbertModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomAlbertModel,self).__init__()\n",
    "        self.bert = AlbertForQuestionAnswering.from_pretrained(weights)\n",
    "        self.bert.train()\n",
    "        \n",
    "    def forward(self, input_ids, token_type_ids=None, attention_mask=None, labels=None):       \n",
    "        token_type_ids = set_segments(input_ids)\n",
    "        outputs = self.bert(input_ids,token_type_ids=token_type_ids)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T15:42:37.861842Z",
     "start_time": "2020-02-02T15:42:37.853410Z"
    }
   },
   "outputs": [],
   "source": [
    "def save_model(learner,output_dir: Path):\n",
    "    def _create_dir(dirc):\n",
    "        if not os.path.exists(dirc): os.mkdir(dirc)\n",
    "    epoch = learner.epoch\n",
    "    metric = round(float(learner.qa_avg_stats.valid_stats.avg_stats[1]),2)\n",
    "    _create_dir(output_dir)\n",
    "    model_dir = f\"{config.model}-accuracy-{metric}-epoch-{epoch}-\" + str(datetime.now())\n",
    "    _create_dir(output_dir/model_dir)\n",
    "    learner.model.bert.save_pretrained(output_dir/model_dir)\n",
    "\n",
    "class SaveModelCallback(Callback):\n",
    "    def __init__(self,save_model_func,output_dir):\n",
    "        self.output_dir, self.save_model_func = output_dir,save_model_func\n",
    "    def after_epoch(self):\n",
    "        self.save_model_func(self, self.output_dir)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T15:42:37.869252Z",
     "start_time": "2020-02-02T15:42:37.864541Z"
    }
   },
   "outputs": [],
   "source": [
    "# gradient accumulation\n",
    "class GradientAccumulation(Callback):\n",
    "    def __init__(self,bs,effective_bs):\n",
    "        self.bs, self.effective_bs = bs, effective_bs\n",
    "    def after_backward(self):\n",
    "        if self.n_iter*self.bs % self.effective_bs != 0: raise CancelBatchException()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T15:42:37.875032Z",
     "start_time": "2020-02-02T15:42:37.871062Z"
    }
   },
   "outputs": [],
   "source": [
    "# defining the loss function\n",
    "def cross_entropy_qa(input, target):\n",
    "    \"\"\"\n",
    "    Summing the cross entropy loss from the starting and ending indices. \n",
    "    \"\"\"\n",
    "    loss = torch.add(F.cross_entropy(input[0], target[:,0]) , F.cross_entropy(input[1], target[:,1]))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T15:42:37.881051Z",
     "start_time": "2020-02-02T15:42:37.876464Z"
    }
   },
   "outputs": [],
   "source": [
    "class QAAvgStats(AvgStats):\n",
    "    def accumulate(self, run):\n",
    "        bn = run.xb.shape[0]\n",
    "        self.tot_loss += run.loss * bn\n",
    "        self.count += bn\n",
    "        for i,m in enumerate(self.metrics):\n",
    "            self.tot_mets[i] += m(run.pred, run.yb, run.xb) * bn\n",
    "            \n",
    "class QAAvgStatsCallback(AvgStatsCallback):\n",
    "    def __init__(self, metrics):\n",
    "        self.train_stats,self.valid_stats = QAAvgStats(metrics,True),QAAvgStats(metrics,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T15:42:37.887556Z",
     "start_time": "2020-02-02T15:42:37.882595Z"
    }
   },
   "outputs": [],
   "source": [
    "# defining the evaluation metrics based on squad evaluation method\n",
    "def acc_qa(input,target,xb):\n",
    "    \"\"\"\n",
    "    Taking the average between the accuracies of predicting the start and ending indices\n",
    "    \"\"\"\n",
    "    return (accuracy(input[0], target[:,0]) + accuracy(input[1], target[:,1]))/2\n",
    "\n",
    "def exact_match(input,target,xb):\n",
    "    def _acc(out, yb): return (torch.argmax(out, dim=1)==yb).float()\n",
    "    return (_acc(input[0], target[:,0]) + _acc(input[1], target[:,1]) == 2).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T15:42:37.896355Z",
     "start_time": "2020-02-02T15:42:37.888776Z"
    }
   },
   "outputs": [],
   "source": [
    "def f1_score(input,target,xb):\n",
    "    \"\"\"\n",
    "    based on the official evaluation script:\n",
    "    https://worksheets.codalab.org/rest/bundles/0x6b567e1cf2e041ec80d7098f031c5c9e/contents/blob/\n",
    "    \"\"\"\n",
    "    pred_starts,pred_ends = [torch.argmax(out, dim=1) for out in input]\n",
    "    gold_starts,gold_ends = target[:,0], target[:,1]\n",
    "    \n",
    "    def _get_toks(idx,start,end):\n",
    "        if start == end: end += 1\n",
    "        return xb[idx][start:end]\n",
    "    \n",
    "    def _score1(pred_toks,gold_toks):\n",
    "        common = collections.Counter(gold_toks.tolist()) & collections.Counter(pred_toks.tolist())\n",
    "        num_same = sum(common.values())\n",
    "        if num_same == 0: \n",
    "            return 0\n",
    "        precision = 1.0 * num_same / len(pred_toks)\n",
    "        recall = 1.0 * num_same / len(gold_toks)\n",
    "        f1 = (2 * precision * recall) / (precision + recall)\n",
    "        return f1\n",
    "    \n",
    "    pred_toks = [_get_toks(i,start,end) for i,(start,end) in enumerate(zip(pred_starts,pred_ends))]\n",
    "    gold_toks = [_get_toks(i,start,end) for i,(start,end) in enumerate(zip(gold_starts,gold_ends))]\n",
    "    score = np.mean([_score1(pred,gold) for pred,gold in zip(pred_toks,gold_toks)])\n",
    "    return score             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T15:42:37.905796Z",
     "start_time": "2020-02-02T15:42:37.897688Z"
    }
   },
   "outputs": [],
   "source": [
    "class MidStatsCallback(Callback):\n",
    "    _order=5\n",
    "    def __init__(self, update_freq_pct=.05):\n",
    "        self.update_freq_pct = update_freq_pct\n",
    "        logging.basicConfig()\n",
    "        self.logger = logging.getLogger()\n",
    "        self.logger.setLevel(logging.INFO)\n",
    "        \n",
    "    def begin_epoch(self):\n",
    "        self.update_freq = int(self.update_freq_pct*len(self.dl))\n",
    "        \n",
    "    def after_batch(self):\n",
    "        if self.n_iter % self.update_freq == 0:\n",
    "            stats = learn.qa_avg_stats.train_stats if self.in_train else learn.qa_avg_stats.valid_stats\n",
    "            self.logger.info(f\"epoch {self.epoch} stats for {self.n_iter} out of {self.iters} are : {stats}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T15:42:37.917020Z",
     "start_time": "2020-02-02T15:42:37.907190Z"
    }
   },
   "outputs": [],
   "source": [
    "cbfs = [partial(QAAvgStatsCallback,[acc_qa,exact_match,]), #f1_score\n",
    "        CudaCallback,\n",
    "        ProgressCallback,\n",
    "        Recorder,\n",
    "       MidStatsCallback]\n",
    "\n",
    "if not config.testing and config.save_checkpoint: \n",
    "    cbfs.append(partial(SaveModelCallback,save_model,config.output_dir))\n",
    "    \n",
    "if config.effective_bs and config.bs != config.effective_bs:\n",
    "    cbfs.append(partial(GradientAccumulation,config.bs,config.effective_bs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T15:42:38.733794Z",
     "start_time": "2020-02-02T15:42:37.918305Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443\n",
      "DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 \"HEAD /models.huggingface.co/bert/albert-base-v2-config.json HTTP/1.1\" 200 0\n",
      "INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/albert-base-v2-config.json from cache at /home/devkosal/.cache/torch/transformers/0bbb1531ce82f042a813219ffeed7a1fa1f44cd8f78a652c47fc5311e0d40231.49ede2f5cbd21a453ab03ed1214f9068f024910f34b5023577f3d0068326f7b0\n",
      "INFO:transformers.configuration_utils:Model config {\n",
      "  \"architectures\": [\n",
      "    \"AlbertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0,\n",
      "  \"down_scale_factor\": 1,\n",
      "  \"embedding_size\": 128,\n",
      "  \"finetuning_task\": null,\n",
      "  \"gap_size\": 0,\n",
      "  \"hidden_act\": \"gelu_new\",\n",
      "  \"hidden_dropout_prob\": 0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"inner_group_num\": 1,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"net_structure_type\": 0,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_groups\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"num_memory_blocks\": 0,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pruned_heads\": {},\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443\n",
      "DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 \"HEAD /models.huggingface.co/bert/albert-base-v2-pytorch_model.bin HTTP/1.1\" 200 0\n",
      "INFO:transformers.modeling_utils:loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/albert-base-v2-pytorch_model.bin from cache at /home/devkosal/.cache/torch/transformers/a175de1d3c60bba6e74bd034c02a34d909d9f36a0cf472b02301c8790ba44834.ab806923413c2af99835e13fdbb6014b24af86b0de8edc2d71ef5c646fc54f24\n",
      "INFO:transformers.modeling_utils:Weights of AlbertForQuestionAnswering not initialized from pretrained model: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "INFO:transformers.modeling_utils:Weights from pretrained model not used in AlbertForQuestionAnswering: ['predictions.bias', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.dense.weight', 'predictions.dense.bias', 'predictions.decoder.weight', 'predictions.decoder.bias']\n"
     ]
    }
   ],
   "source": [
    "model = CustomAlbertModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T15:42:38.738489Z",
     "start_time": "2020-02-02T15:42:38.734936Z"
    }
   },
   "outputs": [],
   "source": [
    "def albert_splitter(m, g1=[],g2=[]):\n",
    "    if \"qa_outputs\" in list(dict(m.named_children()).keys()) :\n",
    "        g2+= m.qa_outputs.parameters()\n",
    "        pass\n",
    "    elif isinstance(m,torch.nn.modules.normalization.LayerNorm):\n",
    "        g2+= m.parameters()\n",
    "    elif hasattr(m, 'weight'): \n",
    "        g1+= m.parameters()\n",
    "    for ll in m.children(): albert_splitter(ll, g1, g2)\n",
    "    return g1,g2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T15:42:38.754032Z",
     "start_time": "2020-02-02T15:42:38.739856Z"
    }
   },
   "outputs": [],
   "source": [
    "# https://github.com/fastai/course-v3/blob/master/nbs/dl2/11_train_imagenette.ipynb\n",
    "def create_phases(phases):\n",
    "    phases = listify(phases)\n",
    "    return phases + [1-sum(phases)]\n",
    "\n",
    "# https://github.com/fastai/course-v3/blob/master/nbs/dl2/11a_transfer_learning.ipynb\n",
    "def sched_1cycle(lrs, pct_start=0.3, mom_start=0.95, mom_mid=0.85, mom_end=0.95):\n",
    "    phases = create_phases(pct_start)\n",
    "    sched_lr  = [combine_scheds(phases, cos_1cycle_anneal(lr/10., lr, lr/1e5))\n",
    "                 for lr in lrs]\n",
    "    sched_mom = combine_scheds(phases, cos_1cycle_anneal(mom_start, mom_mid, mom_end))\n",
    "    return [ParamScheduler('lr', sched_lr),\n",
    "            ParamScheduler('mom', sched_mom)]\n",
    "\n",
    "disc_lr_sched = sched_1cycle([config.max_lr,1e-4], 0.3) # 3e-2 best with adam, 1e-3 for lamb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T15:42:38.762905Z",
     "start_time": "2020-02-02T15:42:38.755128Z"
    }
   },
   "outputs": [],
   "source": [
    "# the learning rate we apply here does not matter since we are scheduling \n",
    "learn = Learner(model, data, cross_entropy_qa,lr=config.max_lr,cb_funcs=cbfs,splitter=albert_splitter,opt_func=config.opt_func)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T17:44:42.649561Z",
     "start_time": "2020-02-02T15:42:38.764085Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>train_acc_qa</th>\n",
       "      <th>train_exact_match</th>\n",
       "      <th>train_f1_score</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>valid_acc_qa</th>\n",
       "      <th>valid_exact_match</th>\n",
       "      <th>valid_f1_score</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:epoch 0 stats for 1626 out of 32526 are : train: [6.073469211254612, tensor(0.3095, device='cuda:0'), tensor(0.1782, device='cuda:0'), 0.31537575342296187]\n",
      "INFO:root:epoch 0 stats for 3252 out of 32526 are : train: [4.702411196475246, tensor(0.4385, device='cuda:0'), tensor(0.2944, device='cuda:0'), 0.4677957324365383]\n",
      "INFO:root:epoch 0 stats for 4878 out of 32526 are : train: [4.121888533338458, tensor(0.4932, device='cuda:0'), tensor(0.3488, device='cuda:0'), 0.5349686068356021]\n",
      "INFO:root:epoch 0 stats for 6504 out of 32526 are : train: [3.758098682061039, tensor(0.5293, device='cuda:0'), tensor(0.3839, device='cuda:0'), 0.5784136100486963]\n",
      "INFO:root:epoch 0 stats for 8130 out of 32526 are : train: [3.5029111892681426, tensor(0.5537, device='cuda:0'), tensor(0.4080, device='cuda:0'), 0.605785311113898]\n",
      "INFO:root:epoch 0 stats for 9756 out of 32526 are : train: [3.315103365877409, tensor(0.5722, device='cuda:0'), tensor(0.4279, device='cuda:0'), 0.6276058993552803]\n",
      "INFO:root:epoch 0 stats for 11382 out of 32526 are : train: [3.179050529344579, tensor(0.5851, device='cuda:0'), tensor(0.4418, device='cuda:0'), 0.6431843234480465]\n",
      "INFO:root:epoch 0 stats for 13008 out of 32526 are : train: [3.078152026637454, tensor(0.5947, device='cuda:0'), tensor(0.4520, device='cuda:0'), 0.6549751254906515]\n",
      "INFO:root:epoch 0 stats for 14634 out of 32526 are : train: [2.989073764435561, tensor(0.6039, device='cuda:0'), tensor(0.4623, device='cuda:0'), 0.6654892266849313]\n",
      "INFO:root:epoch 0 stats for 16260 out of 32526 are : train: [2.9157664994618697, tensor(0.6112, device='cuda:0'), tensor(0.4699, device='cuda:0'), 0.6736519286286677]\n",
      "INFO:root:epoch 0 stats for 17886 out of 32526 are : train: [2.848220806426814, tensor(0.6179, device='cuda:0'), tensor(0.4774, device='cuda:0'), 0.6809761279254932]\n",
      "INFO:root:epoch 0 stats for 19512 out of 32526 are : train: [2.797545260608856, tensor(0.6229, device='cuda:0'), tensor(0.4828, device='cuda:0'), 0.6868230038160567]\n",
      "INFO:root:epoch 0 stats for 21138 out of 32526 are : train: [2.7468299833238716, tensor(0.6277, device='cuda:0'), tensor(0.4882, device='cuda:0'), 0.6923928976214806]\n",
      "INFO:root:epoch 0 stats for 22764 out of 32526 are : train: [2.7067168830170445, tensor(0.6323, device='cuda:0'), tensor(0.4935, device='cuda:0'), 0.6972940460288969]\n",
      "INFO:root:epoch 0 stats for 26016 out of 32526 are : train: [2.6321230204489545, tensor(0.6395, device='cuda:0'), tensor(0.5017, device='cuda:0'), 0.7055339023992523]\n",
      "INFO:root:epoch 0 stats for 27642 out of 32526 are : train: [2.604884550683742, tensor(0.6424, device='cuda:0'), tensor(0.5049, device='cuda:0'), 0.7085977642720452]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "one of the variables needed for gradient computation has been modified by an inplace operation: [torch.cuda.LongTensor [4]] is at version 2; expected version 1 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-82dd035ff0e9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcbs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdisc_lr_sched\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/albert-qa/src/basics.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, epochs, cbs, reset_opt)\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_begin_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_begin_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_batches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/albert-qa/src/basics.py\u001b[0m in \u001b[0;36mall_batches\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0myb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mone_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mCancelEpochException\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'after_cancel_epoch'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/albert-qa/src/basics.py\u001b[0m in \u001b[0;36mone_batch\u001b[0;34m(self, i, xb, yb)\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0myb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'after_loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_train\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0;31m# if not in train, stop function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m                           \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'after_backward'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m                                \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'after_step'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \"\"\"\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.cuda.LongTensor [4]] is at version 2; expected version 1 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True)."
     ]
    }
   ],
   "source": [
    "learn.fit(2,cbs=disc_lr_sched)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T17:44:42.650909Z",
     "start_time": "2020-02-02T15:40:44.321Z"
    }
   },
   "outputs": [],
   "source": [
    "# learn.fit(2,cbs=disc_lr_sched)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T17:44:42.651682Z",
     "start_time": "2020-02-02T15:40:44.323Z"
    }
   },
   "outputs": [],
   "source": [
    "learn.recorder.plot_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T17:44:42.652382Z",
     "start_time": "2020-02-02T15:40:44.324Z"
    }
   },
   "outputs": [],
   "source": [
    "# manually save and load model weights\n",
    "\n",
    "# save_model(learn,config.out_path)\n",
    "# learn.model.bert.from_pretrained(config.output_dir/'albert-base-v2-accuracy-0.72-epoch-3-2020-01-24 16:22:37.693825')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T17:44:42.653176Z",
     "start_time": "2020-02-02T15:40:44.326Z"
    }
   },
   "outputs": [],
   "source": [
    "def pad_collate_x(samples, pad_idx=config.pad_idx, pad_first=False):\n",
    "    max_len = max([len(s[0]) for s in samples])\n",
    "    res = torch.zeros(len(samples), max_len).long() + pad_idx\n",
    "    for i,s in enumerate(samples):\n",
    "        if pad_first: res[i, -len(s[0]):] = torch.LongTensor(s[0])\n",
    "        else:         res[i, :len(s[0]) ] = torch.LongTensor(s[0])\n",
    "    return res\n",
    "\n",
    "def get_pred(text, model, tok):\n",
    "    # 0. Ensure the input is a list for batch processing\n",
    "    text = listify(text)\n",
    "    # 1. tokenize/encode the input text\n",
    "    input_ids = pad_collate_x([torch.tensor(tok.encode(t)).unsqueeze(0) for t in text])\n",
    "    if torch.cuda.is_available(): input_ids, model = input_ids.cuda(), model.cuda()\n",
    "    # 2. extract the logits vector for each item \n",
    "    logits = model(input_ids)\n",
    "    # 3. apply argmax to the logits so we have the probabilities of each item's start and end indices\n",
    "    starts,ends = [torch.argmax(out, dim=1) for out in logits]\n",
    "    \n",
    "    def _proc1(idx,start,end):\n",
    "        # function to process one item at a time\n",
    "        if start > end:\n",
    "            return \"unanswerable\"\n",
    "        elif start == end:\n",
    "            end += 1\n",
    "        pred = tok.convert_ids_to_tokens(input_ids[idx][start:end])\n",
    "        return tok.convert_tokens_to_string(pred)\n",
    "    return [_proc1(idx,start,end) for idx,(start,end) in enumerate(zip(starts,ends))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T17:44:42.653878Z",
     "start_time": "2020-02-02T15:40:44.329Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# tests with samples \n",
    "sample1 = \"[CLS] there have been thirty earthquakes in the past three days. [SEP] how many earthquakes have there been? [SEP]\"\n",
    "sample2 = \"[CLS] the storm took place yesterday from dawn to dusk [SEP] what took place??\"\n",
    "sample3 = \"[CLS] No.2 pencils are used a lot in schools. [SEP] what are used?\"\n",
    "get_pred([sample1,sample2,sample3], learn.model, tok)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
