{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using ALBERT for Question Answering - SQuAD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Updates:\n",
    "\n",
    "Things implemented:\n",
    "- Data Loading\n",
    "- Model\n",
    "- Learner\n",
    "- Prediction on new data\n",
    "- Optimizer selection\n",
    "- Segment IDs\n",
    "- F1 Score\n",
    "- Checkpoints\n",
    "- Use a more powerful machine to train\n",
    "- Gradient Accumulation\n",
    "- Squad 2.0\n",
    "    - Multi Task Learning Changes\n",
    "\n",
    "TODO:\n",
    "- Consider using the sliding window approach for long sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-01T19:01:05.101001Z",
     "start_time": "2020-02-01T19:01:04.190437Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from src import *\n",
    "from transformers import AutoTokenizer, AlbertModel,AlbertPreTrainedModel\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import pickle\n",
    "import re\n",
    "import collections\n",
    "from tqdm import tqdm, trange\n",
    "from datetime import datetime\n",
    "import logging\n",
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-01T19:01:05.104246Z",
     "start_time": "2020-02-01T19:01:05.102425Z"
    }
   },
   "outputs": [],
   "source": [
    "# logging.basicConfig()\n",
    "# logger = logging.getLogger()\n",
    "# logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-01T19:01:05.116234Z",
     "start_time": "2020-02-01T19:01:05.105499Z"
    }
   },
   "outputs": [],
   "source": [
    "config = Config(\n",
    "    data_path = Path(\"../data/SQuAD/2.0\"), # replace with the directory containing the parsed csv files\n",
    "    output_dir = Path(\"./checkpoints\"), # for storing model weights between epochs\n",
    "    task = \"SQuAD\",\n",
    "    squad_version = \"2.0\",\n",
    "    testing=False,\n",
    "    seed = 2020,\n",
    "    model = 'albert-base-v2',\n",
    "    max_lr=5e-5,\n",
    "    max_lr_last = 1e-4,\n",
    "    phases = .3,\n",
    "    optimizer=\"lamb\", # choose between 'adam' or 'lamb'\n",
    "    epochs=2,\n",
    "    use_fp16=False,\n",
    "    recreate_ds=False,\n",
    "    bs=8, \n",
    "    effective_bs=48, # set this different from bs to determine gradient accumulation steps (i.e. effective_bs/bs)\n",
    "    max_seq_len=512,\n",
    "    start_tok = \"[CLS]\",\n",
    "    end_tok = \"[SEP]\",\n",
    "    sep_tok = \"[SEP]\",\n",
    "    unk_tok_idx=1,\n",
    "    sep_idx=3,\n",
    "    pad_idx=0,\n",
    "    feat_cols = [\"question\",\"paragraph\"],\n",
    "    label_cols = [\"idxs\",\"is_impossible\"],\n",
    "    adjustment = 2,\n",
    "    save_checkpoint = True,\n",
    "    load_checkpoint=None#\"albert-base-v2-accuracy-0.72-epoch-0-2020-01-24 18:54:15.308899\"\n",
    ")\n",
    "\n",
    "config.model_name = re.findall(r\"(.+?)-\",config.model)[0]\n",
    "\n",
    "# set optimizer\n",
    "assert config.optimizer.lower() in [\"adam\",\"lamb\"], f\"invalid optimizer in config {config.optimizer}\"\n",
    "config.opt_func = lamb_opt() if config.optimizer.lower() == \"lamb\" else adam_opt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-01T19:01:05.123090Z",
     "start_time": "2020-02-01T19:01:05.117540Z"
    }
   },
   "outputs": [],
   "source": [
    "# utility functions\n",
    "def remove_max_sl(df):\n",
    "    init_len = len(df)\n",
    "    df = df[df.seq_len < config.max_seq_len-2]\n",
    "    new_len = len(df)\n",
    "    print(f\"dropping {init_len - new_len} out of {init_len} questions\")\n",
    "    return df\n",
    "\n",
    "def str2tensor(s):\n",
    "    indices = re.findall(\"-?\\d+\",s)\n",
    "    return torch.tensor([int(indices[0]), int(indices[1])], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-01T19:01:06.109687Z",
     "start_time": "2020-02-01T19:01:05.124303Z"
    }
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(config.data_path/f\"train_{config.squad_version}_{config.model_name}.csv\")\n",
    "valid = pd.read_csv(config.data_path/f\"val_{config.squad_version}_{config.model_name}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-01T19:01:06.394535Z",
     "start_time": "2020-02-01T19:01:06.110885Z"
    }
   },
   "outputs": [],
   "source": [
    "train.drop_duplicates(inplace=True)\n",
    "valid.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-01T19:01:06.453427Z",
     "start_time": "2020-02-01T19:01:06.395725Z"
    }
   },
   "outputs": [],
   "source": [
    "# randomizing the order of training data\n",
    "train = train.sample(frac=1,random_state = config.seed).reset_index(drop=True)\n",
    "valid = valid.sample(frac=1, random_state = config.seed).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-01T19:01:06.457877Z",
     "start_time": "2020-02-01T19:01:06.455485Z"
    }
   },
   "outputs": [],
   "source": [
    "# reduce df sizes if testing\n",
    "if config.testing:\n",
    "    train = train[:int(len(train)/100)]\n",
    "    valid = valid[:int(len(valid)/100)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-01T19:01:06.488166Z",
     "start_time": "2020-02-01T19:01:06.459452Z"
    }
   },
   "outputs": [],
   "source": [
    "train, valid = remove_max_sl(train), remove_max_sl(valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-01T19:01:06.491220Z",
     "start_time": "2020-02-01T19:01:06.489383Z"
    }
   },
   "outputs": [],
   "source": [
    "# train[\"idxs\"] = train.apply(lambda row: row[\"idxs\"] if not row[\"is_impossible\"] else '[-2, -2]',axis=1)\n",
    "# valid[\"idxs\"] = valid.apply(lambda row: row[\"idxs\"] if not row[\"is_impossible\"] else '[-2, -2]',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-01T19:01:06.506785Z",
     "start_time": "2020-02-01T19:01:06.492432Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up the Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-01T19:01:06.514955Z",
     "start_time": "2020-02-01T19:01:06.508171Z"
    }
   },
   "outputs": [],
   "source": [
    "class TokenizerProcessor(Processor):\n",
    "    def __init__(self, tok_func, max_sl, start_tok, end_tok, pre_rules=None,post_rules=None):\n",
    "        self.tok_func,self.max_sl = tok_func,max_sl\n",
    "        self.pre_rules,self.post_rules=pre_rules,post_rules\n",
    "        self.start_tok, self.end_tok = start_tok, end_tok\n",
    "\n",
    "    def proc1(self, x): return [self.start_tok] + self.tok_func(x)[:self.max_sl-2] + [self.end_tok]\n",
    "    \n",
    "    def __call__(self, items): return tqdm([self.proc1(x) for x in items])\n",
    "\n",
    "class NumericalizeProcessor(Processor):\n",
    "    \"\"\"\n",
    "    only works with an existing vocab at the moment and min_freq is not accounted for\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab:dict, unk_tok_idx:int, min_freq=2): \n",
    "        self.vocab, self.unk_tok_idx, self.min_freq = vocab, unk_tok_idx, min_freq\n",
    "    \n",
    "    def proc1(self, x): return [self.vocab[i] if i in self.vocab else self.unk_tok_idx for i in x]\n",
    "    \n",
    "    def __call__(self, items): \n",
    "        if getattr(self, 'otoi', None) is None:\n",
    "            self.otoi = collections.defaultdict(int,{v:k for k,v in enumerate(self.vocab)})\n",
    "        return tqdm([self.proc1(x) for x in items])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-01T19:01:06.704350Z",
     "start_time": "2020-02-01T19:01:06.516042Z"
    }
   },
   "outputs": [],
   "source": [
    "tok = AutoTokenizer.from_pretrained(config.model)\n",
    "proc_tok = TokenizerProcessor(tok.tokenize, config.max_seq_len, config.start_tok, config.end_tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-01T19:01:06.748353Z",
     "start_time": "2020-02-01T19:01:06.707289Z"
    }
   },
   "outputs": [],
   "source": [
    "vocab = {tok.convert_ids_to_tokens(i):i for i in range(tok.vocab_size)}\n",
    "proc_num = NumericalizeProcessor(vocab, unk_tok_idx=config.unk_tok_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-01T19:01:06.761144Z",
     "start_time": "2020-02-01T19:01:06.749586Z"
    }
   },
   "outputs": [],
   "source": [
    "class QALabelProcessor(Processor):\n",
    "    def __init__(self, parse_func = noop, adjustment = 2):\n",
    "        self.parse_func = parse_func\n",
    "        self.adjustment = adjustment\n",
    "        self.vocab=None\n",
    "        \n",
    "    def cat_proc1(self,item): return self.otoi[item]\n",
    "    def index_proc1(self, item): return self.parse_func(item) + self.adjustment\n",
    "    \n",
    "    def __call__(self, items): \n",
    "        if self.vocab is None:\n",
    "            self.vocab = uniqueify([o[1] for o in items])\n",
    "            self.otoi  = {v:k for k,v in enumerate(self.vocab)}\n",
    "        return [(self.index_proc1(item[0]),self.cat_proc1(item[1])) for item in items] # we want the \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-01T19:01:06.766573Z",
     "start_time": "2020-02-01T19:01:06.762344Z"
    }
   },
   "outputs": [],
   "source": [
    "proc_qa = QALabelProcessor(str2tensor,config.adjustment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-01T19:01:06.773020Z",
     "start_time": "2020-02-01T19:01:06.767772Z"
    }
   },
   "outputs": [],
   "source": [
    "class SquadTextList(ItemList):      \n",
    "    @classmethod  \n",
    "    def from_df(cls, df, feat_cols, label_cols, sep_tok, test=False):\n",
    "        feat_cols = listify(feat_cols)\n",
    "        x = df[feat_cols[0]]\n",
    "        for i in range(1,len(feat_cols)):\n",
    "            x += f\" {sep_tok} \" + df[feat_cols[i]]\n",
    "        labels = cls(df[label_cols].values) if not test else cls([[None,None] for _ in len(df)])\n",
    "        return cls(x,labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-01T19:03:05.820959Z",
     "start_time": "2020-02-01T19:01:06.774167Z"
    }
   },
   "outputs": [],
   "source": [
    "if (not (config.data_path/f\"squad_{config.squad_version}_data_trn.pkl\").exists()) or config.recreate_ds or config.testing:\n",
    "    il_train = SquadTextList.from_df(train,config.feat_cols,config.label_cols,config.sep_tok)\n",
    "    il_valid = SquadTextList.from_df(valid,config.feat_cols,config.label_cols,config.sep_tok)\n",
    "\n",
    "    ll_valid = LabeledData(il_valid,il_valid.labels,proc_x = [proc_tok,proc_num], proc_y=[proc_qa])\n",
    "    ll_train = LabeledData(il_train,il_train.labels,proc_x = [proc_tok,proc_num], proc_y=[proc_qa])\n",
    "\n",
    "    # saving/loading presaved data if not testing\n",
    "    if not config.testing:\n",
    "        # save an object\n",
    "        pickle.dump(ll_train, open( config.data_path/f\"squad_{config.squad_version}_data_trn.pkl\", \"wb\" ) )\n",
    "        pickle.dump(ll_valid, open( config.data_path/f\"squad_{config.squad_version}_data_val.pkl\", \"wb\" ) )\n",
    "else:\n",
    "    # load an object\n",
    "    ll_train = pickle.load( open( config.data_path/f\"squad_{config.squad_version}_data_trn.pkl\", \"rb\" ) )\n",
    "    ll_valid = pickle.load( open( config.data_path/f\"squad_{config.squad_version}_data_val.pkl\", \"rb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-01T19:03:05.825724Z",
     "start_time": "2020-02-01T19:03:05.822210Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Sampler\n",
    "\n",
    "class SortSampler(Sampler):\n",
    "    def __init__(self, data_source, key): self.data_source,self.key = data_source,key\n",
    "    def __len__(self): return len(self.data_source)\n",
    "    def __iter__(self):\n",
    "        return iter(sorted(list(range(len(self.data_source))), key=self.key, reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-01T19:03:05.834179Z",
     "start_time": "2020-02-01T19:03:05.827106Z"
    }
   },
   "outputs": [],
   "source": [
    "class SortishSampler(Sampler):\n",
    "    def __init__(self, data_source, key, bs):\n",
    "        self.data_source,self.key,self.bs = data_source,key,bs\n",
    "\n",
    "    def __len__(self) -> int: return len(self.data_source)\n",
    "\n",
    "    def __iter__(self):\n",
    "        idxs = torch.randperm(len(self.data_source))\n",
    "        megabatches = [idxs[i:i+self.bs*50] for i in range(0, len(idxs), self.bs*50)]\n",
    "        sorted_idx = torch.cat([tensor(sorted(s, key=self.key, reverse=True)) for s in megabatches])\n",
    "        batches = [sorted_idx[i:i+self.bs] for i in range(0, len(sorted_idx), self.bs)]\n",
    "        max_idx = torch.argmax(tensor([self.key(ck[0]) for ck in batches]))  # find the chunk with the largest key,\n",
    "        batches[0],batches[max_idx] = batches[max_idx],batches[0]            # then make sure it goes first.\n",
    "        batch_idxs = torch.randperm(len(batches)-2)\n",
    "        sorted_idx = torch.cat([batches[i+1] for i in batch_idxs]) if len(batches) > 1 else LongTensor([])\n",
    "        sorted_idx = torch.cat([batches[0], sorted_idx, batches[-1]])\n",
    "        return iter(sorted_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-01T19:03:05.841683Z",
     "start_time": "2020-02-01T19:03:05.835464Z"
    }
   },
   "outputs": [],
   "source": [
    "def pad_collate_qa(samples, pad_idx=config.pad_idx, pad_first=False):\n",
    "    max_len = max([len(s[0]) for s in samples])\n",
    "    res = torch.zeros(len(samples), max_len).long() + pad_idx\n",
    "    for i,s in enumerate(samples):\n",
    "        if pad_first: res[i, -len(s[0]):] = torch.LongTensor(s[0])\n",
    "        else:         res[i, :len(s[0]) ] = torch.LongTensor(s[0])\n",
    "    qa_idxs = torch.cat([s[1][0].unsqueeze(0) for s in samples])\n",
    "    imp_labels = torch.tensor([s[1][1] for s in samples])\n",
    "    return res, (qa_idxs, imp_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-01T19:03:05.847520Z",
     "start_time": "2020-02-01T19:03:05.842796Z"
    }
   },
   "outputs": [],
   "source": [
    "train_sampler = SortishSampler(ll_train.x, key=lambda t: len(ll_train[int(t)][0]), bs=config.bs)\n",
    "train_dl = DataLoader(ll_train, batch_size=config.bs, sampler=train_sampler, collate_fn=pad_collate_qa)\n",
    "\n",
    "valid_sampler = SortSampler(ll_valid.x, key=lambda t: len(ll_valid[int(t)][0]))\n",
    "valid_dl = DataLoader(ll_valid, batch_size=config.bs, sampler=valid_sampler, collate_fn=pad_collate_qa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-01T19:03:05.853049Z",
     "start_time": "2020-02-01T19:03:05.849893Z"
    }
   },
   "outputs": [],
   "source": [
    "# x,y = next(iter(train_dl))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up the Databunch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-01T19:03:05.862352Z",
     "start_time": "2020-02-01T19:03:05.854454Z"
    }
   },
   "outputs": [],
   "source": [
    "data = DataBunch(train_dl,valid_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-01T19:03:05.871505Z",
     "start_time": "2020-02-01T19:03:05.863494Z"
    }
   },
   "outputs": [],
   "source": [
    "def set_segments(x,sep_idx=config.sep_idx):\n",
    "    res = x.new_zeros(x.size())\n",
    "    for row_idx, row in enumerate(x):\n",
    "        in_seg_1 = False\n",
    "        for val_idx,val in enumerate(row):\n",
    "            if val == sep_idx:\n",
    "                in_seg_1 = True\n",
    "            if in_seg_1: \n",
    "                res[row_idx,val_idx] = 1\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-01T19:03:05.883178Z",
     "start_time": "2020-02-01T19:03:05.872614Z"
    }
   },
   "outputs": [],
   "source": [
    "weights = config.output_dir/config.load_checkpoint if config.load_checkpoint else config.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-01T19:03:05.892617Z",
     "start_time": "2020-02-01T19:03:05.884330Z"
    }
   },
   "outputs": [],
   "source": [
    "class AlbertForQuestionAnsweringMTL(AlbertPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "\n",
    "        self.bert = AlbertModel(config)\n",
    "        self.qa_outputs = nn.Linear(config.hidden_size, config.num_labels)\n",
    "        self.imp_outputs = nn.Linear(config.hidden_size, 2)\n",
    "        self.imp_dropout = nn.Dropout(.1)\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(self, input_ids=None, token_type_ids=None, position_ids=None):\n",
    "\n",
    "        bert_outputs = self.bert(input_ids=input_ids, token_type_ids=token_type_ids, position_ids=position_ids)\n",
    "\n",
    "        sequence_output = bert_outputs[0]\n",
    "        pooled_output = bert_outputs[1]\n",
    "\n",
    "\n",
    "        logits_qa = self.qa_outputs(sequence_output)\n",
    "        start_logits, end_logits = logits_qa.split(1, dim=-1)\n",
    "        start_logits = start_logits.squeeze(-1)\n",
    "        end_logits = end_logits.squeeze(-1)\n",
    "    \n",
    "        imp_logits = self.imp_outputs(self.imp_dropout(pooled_output))\n",
    "\n",
    "        outputs = (start_logits, end_logits, imp_logits,)\n",
    "        return outputs  # start_logits, end_logits, (hidden_states), (attentions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AlbertForQuestionAnsweringMTL.from_pretrained(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Function and Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-01T19:03:05.929403Z",
     "start_time": "2020-02-01T19:03:05.926416Z"
    }
   },
   "outputs": [],
   "source": [
    "def assert_no_negs(tensor):\n",
    "    assert torch.all(torch.eq(tensor, abs(tensor)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-01T19:03:05.935449Z",
     "start_time": "2020-02-01T19:03:05.930425Z"
    }
   },
   "outputs": [],
   "source": [
    "# defining the loss function\n",
    "def cross_entropy_qa_mtl(input, target):\n",
    "    \"\"\"\n",
    "    Summing the cross entropy loss from the starting and ending indices. \n",
    "    \"\"\"\n",
    "    assert_no_negs(target[0])\n",
    "    assert_no_negs(target[1])\n",
    "    \n",
    "    mask = (~target[1].bool()).float()\n",
    "    qa_loss = torch.add(F.cross_entropy(input[0], target[0][:,0], reduction=\"none\"),\\\n",
    "                        F.cross_entropy(input[1], target[0][:,1], reduction=\"none\"))/2.0\n",
    "    qa_loss.mul_(mask+1)\n",
    "    wtd_qa_loss = qa_loss.mean()\n",
    "\n",
    "    imp_loss = F.cross_entropy(input[2], target[1])\n",
    "    \n",
    "    loss = torch.add(wtd_qa_loss, imp_loss)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-01T19:03:05.950457Z",
     "start_time": "2020-02-01T19:03:05.942416Z"
    }
   },
   "outputs": [],
   "source": [
    "# defining the evaluation metrics based on squad evaluation method\n",
    "def acc_qa(input,target,xb):\n",
    "    \"\"\"\n",
    "    Taking the average between the accuracies of predicting the start and ending indices\n",
    "    \"\"\"\n",
    "    return (accuracy(input[0], target[0][:,0]) + accuracy(input[1], target[0][:,1]))\n",
    "\n",
    "def acc_imp(input,target,xb):\n",
    "    return accuracy(input[2], target[1])\n",
    "\n",
    "def exact_match(input,target,xb):\n",
    "    def _acc(out, yb): return (torch.argmax(out, dim=1)==yb).float()\n",
    "    return (_acc(input[0], target[0][:,0]) + _acc(input[1], target[0][:,1]) == 2).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-01T19:03:05.960000Z",
     "start_time": "2020-02-01T19:03:05.951540Z"
    }
   },
   "outputs": [],
   "source": [
    "def f1_score(input,target,xb):\n",
    "    \"\"\"\n",
    "    based on the official evaluation script:\n",
    "    https://worksheets.codalab.org/rest/bundles/0x6b567e1cf2e041ec80d7098f031c5c9e/contents/blob/\n",
    "    \"\"\"\n",
    "    pred_starts,pred_ends = [torch.argmax(out, dim=1) for out in input[:2]]\n",
    "    gold_starts,gold_ends = target[0][:,0], target[0][:,1]\n",
    "    \n",
    "    def _get_toks(idx,start,end):\n",
    "        if start == end: end += 1\n",
    "        return xb[idx][start:end]\n",
    "    \n",
    "    def _score1(pred_toks,gold_toks):\n",
    "        common = collections.Counter(gold_toks.tolist()) & collections.Counter(pred_toks.tolist())\n",
    "        num_same = sum(common.values())\n",
    "        if num_same == 0: \n",
    "            return 0\n",
    "        precision = 1.0 * num_same / len(pred_toks)\n",
    "        recall = 1.0 * num_same / len(gold_toks)\n",
    "        f1 = (2 * precision * recall) / (precision + recall)\n",
    "        return f1\n",
    "    \n",
    "    pred_toks = [_get_toks(i,start,end) for i,(start,end) in enumerate(zip(pred_starts,pred_ends))]\n",
    "    gold_toks = [_get_toks(i,start,end) for i,(start,end) in enumerate(zip(gold_starts,gold_ends))]\n",
    "    score = np.mean([_score1(pred,gold) for pred,gold in zip(pred_toks,gold_toks)])\n",
    "    return score             "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-01T19:03:05.902214Z",
     "start_time": "2020-02-01T19:03:05.893744Z"
    }
   },
   "outputs": [],
   "source": [
    "def save_model(learner,output_dir: Path):\n",
    "    def _create_dir(dirc):\n",
    "        if not os.path.exists(dirc): os.mkdir(dirc)\n",
    "    epoch = learner.epoch\n",
    "    metric = round(float(learner.qa_avg_stats.valid_stats.avg_stats[1]),2)\n",
    "    _create_dir(output_dir)\n",
    "    model_dir = f\"{config.model}-accuracy-{metric}-epoch-{epoch}-\" + str(datetime.now())\n",
    "    _create_dir(output_dir/model_dir)\n",
    "    learner.model.save_pretrained(output_dir/model_dir) # learner.model.bert.save...\n",
    "\n",
    "class SaveModelCallback(Callback):\n",
    "    def __init__(self,save_model_func,output_dir):\n",
    "        self.output_dir, self.save_model_func = output_dir,save_model_func\n",
    "    def after_epoch(self):\n",
    "        self.save_model_func(self, self.output_dir)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-01T19:03:05.912689Z",
     "start_time": "2020-02-01T19:03:05.903326Z"
    }
   },
   "outputs": [],
   "source": [
    "class CudaCallbackMTL(Callback):\n",
    "    def begin_fit(self): self.model.cuda()\n",
    "    def begin_batch(self): self.run.xb, self.run.yb = \\\n",
    "        self.xb.cuda(), (self.run.yb[0].cuda(), self.run.yb[1].cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-01T19:03:05.918510Z",
     "start_time": "2020-02-01T19:03:05.913765Z"
    }
   },
   "outputs": [],
   "source": [
    "# gradient accumulation\n",
    "class GradientAccumulation(Callback):\n",
    "    _order=2\n",
    "    def __init__(self,bs,effective_bs):\n",
    "        self.bs, self.effective_bs = bs, effective_bs\n",
    "    def after_loss(self):\n",
    "        self.loss.div_(self.effective_bs/self.bs)\n",
    "    def after_backward(self):\n",
    "        if self.n_iter*self.bs % self.effective_bs != 0: raise CancelBatchException()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QAAvgStats(AvgStats):\n",
    "    def accumulate(self, run):\n",
    "        bn = run.xb.shape[0]\n",
    "        self.tot_loss += run.loss * bn\n",
    "        self.count += bn\n",
    "        for i,m in enumerate(self.metrics):\n",
    "            self.tot_mets[i] += m(run.pred, run.yb, run.xb) * bn\n",
    "            \n",
    "class QAAvgStatsCallback(AvgStatsCallback):\n",
    "    def __init__(self, metrics):\n",
    "        self.train_stats,self.valid_stats = QAAvgStats(metrics,True),QAAvgStats(metrics,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MidStatsCallback(Callback):\n",
    "    _order=1\n",
    "    def __init__(self,update_freq=None):\n",
    "        if update_freq is None: self.update_freq = int(.1*self.iters)\n",
    "        logging.basicConfig()\n",
    "        self.logger = logging.getLogger()\n",
    "        self.logger.setLevel(logging.INFO)\n",
    "        \n",
    "    def after_batch(self):\n",
    "        if self.iters % self.update_freq == 0:\n",
    "            stats = learn.qa_avg_stats.train_stats if self.in_train else learn.qa_avg_stats.valid_stats\n",
    "            self.logger.info(f\"epoch {self.epoch} stats for {self.n_iter} out of {self.iters} are : {stats}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-01T19:03:05.971279Z",
     "start_time": "2020-02-01T19:03:05.961104Z"
    }
   },
   "outputs": [],
   "source": [
    "cbfs = [partial(QAAvgStatsCallback,[acc_qa,acc_imp,exact_match]),#,exact_match,,f1_score]),\n",
    "        CudaCallbackMTL,\n",
    "        ProgressCallback,\n",
    "        Recorder,\n",
    "       MidStatsCallback]\n",
    "\n",
    "if not config.testing and config.save_checkpoint: \n",
    "    cbfs.append(partial(SaveModelCallback,save_model,config.output_dir))\n",
    "    \n",
    "if config.effective_bs and config.bs != config.effective_bs:\n",
    "    cbfs.append(partial(GradientAccumulation,config.bs,config.effective_bs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-01T19:03:06.710119Z",
     "start_time": "2020-02-01T19:03:06.706206Z"
    }
   },
   "outputs": [],
   "source": [
    "def albert_splitter(m, g1=[],g2=[]):\n",
    "    l = list(dict(m.named_children()).keys())\n",
    "    if \"qa_outputs\" in  l: g2+= m.qa_outputs.parameters()\n",
    "    if \"imp_outputs\" in l: g2+= m.imp_outputs.parameters()\n",
    "    if isinstance(m,torch.nn.modules.normalization.LayerNorm):\n",
    "        g1+= m.parameters()\n",
    "    elif hasattr(m, 'weight'): \n",
    "        g1+= m.parameters()\n",
    "    for ll in m.children(): albert_splitter(ll, g1, g2)\n",
    "    return g1,g2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LR Scheduling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-01T19:03:06.728664Z",
     "start_time": "2020-02-01T19:03:06.711029Z"
    }
   },
   "outputs": [],
   "source": [
    "# https://github.com/fastai/course-v3/blob/master/nbs/dl2/11_train_imagenette.ipynb\n",
    "def create_phases(phases):\n",
    "    phases = listify(phases)\n",
    "    return phases + [1-sum(phases)]\n",
    "\n",
    "# https://github.com/fastai/course-v3/blob/master/nbs/dl2/11a_transfer_learning.ipynb\n",
    "def sched_1cycle(lrs, pct_start=0.3, mom_start=0.95, mom_mid=0.85, mom_end=0.95):\n",
    "    phases = create_phases(pct_start)\n",
    "    sched_lr  = [combine_scheds(phases, cos_1cycle_anneal(lr/10., lr, lr/1e5))\n",
    "                 for lr in lrs]\n",
    "    sched_mom = combine_scheds(phases, cos_1cycle_anneal(mom_start, mom_mid, mom_end))\n",
    "    return [ParamScheduler('lr', sched_lr),\n",
    "            ParamScheduler('mom', sched_mom)]\n",
    "\n",
    "disc_lr_sched = sched_1cycle([config.max_lr,config.max_lr_last], config.phases) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-01T19:03:06.734842Z",
     "start_time": "2020-02-01T19:03:06.729901Z"
    }
   },
   "outputs": [],
   "source": [
    "# the learning rate we apply here does not matter since we are scheduling \n",
    "learn = Learner(model, data, cross_entropy_qa_mtl,lr=config.max_lr,cb_funcs=cbfs,splitter=albert_splitter\\\n",
    "                ,opt_func=config.opt_func)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-02-01T19:01:04.290Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "learn.fit(9,cbs=disc_lr_sched)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.is_impossible.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.is_impossible[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-02-01T19:01:04.292Z"
    }
   },
   "outputs": [],
   "source": [
    "learn.qa_avg_stats.train_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-02-01T19:01:04.293Z"
    }
   },
   "outputs": [],
   "source": [
    "learn.recorder.plot_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-02-01T19:01:04.295Z"
    }
   },
   "outputs": [],
   "source": [
    "# manually save and load model weights\n",
    "\n",
    "# save_model(learn,config.out_path)\n",
    "# learn.model.bert.from_pretrained(config.output_dir/'albert-base-v2-accuracy-0.72-epoch-3-2020-01-24 16:22:37.693825')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-02-01T19:01:04.297Z"
    }
   },
   "outputs": [],
   "source": [
    "def pad_collate_x(samples, pad_idx=config.pad_idx, pad_first=False):\n",
    "    max_len = max([len(s[0]) for s in samples])\n",
    "    res = torch.zeros(len(samples), max_len).long() + pad_idx\n",
    "    for i,s in enumerate(samples):\n",
    "        if pad_first: res[i, -len(s[0]):] = torch.LongTensor(s[0])\n",
    "        else:         res[i, :len(s[0]) ] = torch.LongTensor(s[0])\n",
    "    return res\n",
    "\n",
    "def prep_text(text, question, tok):\n",
    "    tok_text, tok_ques = tok.tokenize(text), tok.tokenize(question) \n",
    "    truncate_len = 512 - len(tok_ques) - 3*3\n",
    "    res = [\"[CLS]\"] + tok_text[:truncate_len] + [\"[SEP]\"] + tok_ques + [\"[SEP]\"]\n",
    "    return torch.tensor(tok.convert_tokens_to_ids(res)).unsqueeze(0)\n",
    "\n",
    "def get_pred(texts, question, model, tok):\n",
    "    if texts == []: return \"could not find a section which matched query\",\"N/A\"\n",
    "    texts = listify(texts)\n",
    "    # 1. tokenize/encode the input text\n",
    "    input_ids = pad_collate_x([prep_text(t, question, tok) for t in texts])\n",
    "    # 2. extract the logits vector for the next possible token\n",
    "    outputs = model(input_ids.cuda())\n",
    "    logits,imp_logits = outputs[:2],outputs[2]\n",
    "    # 3. apply argmax to the logits so we have the probabilities of each index \n",
    "    (start_probs,starts),(end_probs,ends) = [torch.max(out, dim=1) for out in logits]\n",
    "    # 4. sort the sums of the starts and ends to determine which answers are the most ideal\n",
    "    sorted_sums = np.argsort([sp+ep for (sp,ep) in zip(start_probs,end_probs)])[::-1]\n",
    "    answerable = bool(torch.argmax(imp_logits,dim=1))\n",
    "    def _proc1(idx,start,end):\n",
    "        if start > end: return\n",
    "        elif start == end: end += 1\n",
    "        pred = tok.convert_ids_to_tokens(input_ids[idx][start:end])\n",
    "        return tok.convert_tokens_to_string(pred)\n",
    "    \n",
    "    # find the best answer\n",
    "    for i,s in enumerate(sorted_sums):\n",
    "        ans = _proc1(i,starts[i],ends[i])\n",
    "        if ans is not None and \"<pad>\" not in ans: return answerable, ans, texts[i]\n",
    "    return \"unanswerable\",texts[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-02-01T19:01:04.298Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# tests with samples \n",
    "sample1 = \"[CLS] there have been thirty earthquakes in the past three days. [SEP] how many earthquakes have there been? [SEP]\"\n",
    "sample2 = \"[CLS] the storm took place yesterday from dawn to dusk [SEP] what took place??\"\n",
    "sample3 = \"[CLS] No.2 pencils are used a lot in schools. [SEP] what are used?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-02-01T19:01:04.299Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "get_pred([sample3],\"\",learn.model,tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (qa)",
   "language": "python",
   "name": "qa"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
